{"cells":[{"cell_type":"markdown","metadata":{"id":"Ft_joxkFuf9T"},"source":["## Install the neccessary dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0WwwljUq1npc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers.git@main\n","  Cloning https://github.com/huggingface/transformers.git (to revision main) to /tmp/pip-req-build-az79cf4r\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-az79cf4r\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 14.3 MB/s \n","\u001b[?25hCollecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 10.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.64.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (1.21.6)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (3.7.1)\n","Collecting tokenizers!=0.11.3,\u003c0.13,\u003e=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 64.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.11.4)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers==4.21.0.dev0) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers==4.21.0.dev0) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers==4.21.0.dev0) (3.8.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.21.0.dev0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.21.0.dev0) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.21.0.dev0) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.21.0.dev0) (2.10)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.21.0.dev0-py3-none-any.whl size=4447017 sha256=61fc1773e150667ee672e7266d8aebc3af0ea8e56bc1f9820c91f1aa7f6ebc0c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-dn0renyt/wheels/b9/f5/17/554bf0fb3fc6b3921def9c39b83cd1bf8576649f1dd985e8aa\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0.dev0\n"]}],"source":["!pip install git+https://github.com/huggingface/transformers.git@main"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QB1TivRo_DOy"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 14.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2678,"status":"ok","timestamp":1655314954225,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"fmN7nx8W0zqS","outputId":"47ff9a3e-d7ca-4f84-b808-26eb65be57b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (3.0.4)\n"]}],"source":["! pip install chardet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2743,"status":"ok","timestamp":1655315028340,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"QgQKCJ660zqS","outputId":"45793f80-51cd-40c1-d728-286b07762b6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.7.0)\n","Requirement already satisfied: nbformat\u003e=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.4.0)\n","Requirement already satisfied: traitlets\u003e=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (0.2.0)\n","Requirement already satisfied: jupyterlab-widgets\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.1.0)\n","Requirement already satisfied: ipython\u003e=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n","Requirement already satisfied: ipykernel\u003e=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.6.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel\u003e=4.5.1-\u003eipywidgets) (5.3.5)\n","Requirement already satisfied: tornado\u003e=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel\u003e=4.5.1-\u003eipywidgets) (5.1.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets) (4.4.2)\n","Requirement already satisfied: prompt-toolkit\u003c2.0.0,\u003e=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets) (1.0.18)\n","Requirement already satisfied: setuptools\u003e=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets) (57.4.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets) (2.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets) (0.7.5)\n","Requirement already satisfied: simplegeneric\u003e0.8 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets) (0.8.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat\u003e=4.2.0-\u003eipywidgets) (4.10.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat\u003e=4.2.0-\u003eipywidgets) (2.15.3)\n","Requirement already satisfied: jsonschema\u003e=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat\u003e=4.2.0-\u003eipywidgets) (4.3.3)\n","Requirement already satisfied: importlib-resources\u003e=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema\u003e=2.6-\u003enbformat\u003e=4.2.0-\u003eipywidgets) (5.7.1)\n","Requirement already satisfied: attrs\u003e=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema\u003e=2.6-\u003enbformat\u003e=4.2.0-\u003eipywidgets) (21.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,\u003e=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema\u003e=2.6-\u003enbformat\u003e=4.2.0-\u003eipywidgets) (0.18.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema\u003e=2.6-\u003enbformat\u003e=4.2.0-\u003eipywidgets) (4.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema\u003e=2.6-\u003enbformat\u003e=4.2.0-\u003eipywidgets) (4.11.4)\n","Requirement already satisfied: zipp\u003e=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources\u003e=1.4.0-\u003ejsonschema\u003e=2.6-\u003enbformat\u003e=4.2.0-\u003eipywidgets) (3.8.0)\n","Requirement already satisfied: six\u003e=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit\u003c2.0.0,\u003e=1.0.4-\u003eipython\u003e=4.0.0-\u003eipywidgets) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit\u003c2.0.0,\u003e=1.0.4-\u003eipython\u003e=4.0.0-\u003eipywidgets) (0.2.5)\n","Requirement already satisfied: notebook\u003e=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0-\u003eipywidgets) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (1.8.0)\n","Requirement already satisfied: terminado\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (0.13.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (2.11.3)\n","Requirement already satisfied: pyzmq\u003e=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-\u003eipykernel\u003e=4.5.1-\u003eipywidgets) (23.1.0)\n","Requirement already satisfied: python-dateutil\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-\u003eipykernel\u003e=4.5.1-\u003eipywidgets) (2.8.2)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado\u003e=0.8.1-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (0.7.0)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (2.0.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (5.0.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (0.6.0)\n","Requirement already satisfied: entrypoints\u003e=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (0.4)\n","Requirement already satisfied: mistune\u003c2,\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (0.8.4)\n","Requirement already satisfied: pandocfilters\u003e=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (1.5.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (0.7.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-\u003enbconvert-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets) (0.5.1)\n"]}],"source":["!pip3 install ipywidgets --user"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tT2Qf7KYevoV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kodW7bCufDJx"},"outputs":[],"source":["%cd /gdrive/My\\ Drive/NLP_Project"]},{"cell_type":"markdown","metadata":{"id":"VGQ123KMunAh"},"source":["## Switch to GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMoaEAFDReoY"},"outputs":[],"source":["import torch\n","from tqdm.notebook import tqdm\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655318438451,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"muK1nsQ70zqU","outputId":"5278ea1a-5ac7-4815-a5b7-16ace7d518f8"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda:0'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"markdown","metadata":{"id":"zV1oHTJ618sD"},"source":["## Read the original files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWhUfkaYPuFf"},"outputs":[],"source":["sentences_vi = []\n","with open(\"./data/augmented/back_translation/back_translation_data.txt\", encoding = 'utf-8') as f:\n","  for line in f.readlines():\n","        sentences_vi.append(line)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1655318441488,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"YSr7MeKGRV9j","outputId":"d233d9cb-9818-4ed1-d6a3-0da3005e362b"},"outputs":[{"name":"stdout","output_type":"stream","text":["65000\n"]}],"source":["# print(len(sentences_en))\n","print(len(sentences_vi))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32UJiz63fiCK"},"outputs":[],"source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","import torch\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17710,"status":"ok","timestamp":1655318481048,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"hPkooPXDfu7W","outputId":"e96322c3-433a-490f-c792-a22e83eb03f8"},"outputs":[{"data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(250112, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(250112, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(250112, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-vi-en-base\")\n","tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-vi-en-base\")\n","model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"executionInfo":{"elapsed":514,"status":"error","timestamp":1655318486198,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"81-xOB5if8AW","outputId":"c7df2dfc-7bd8-4cbe-d17e-069e6b626013"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"\u003cipython-input-9-dfa2b50a65f7\u003e\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    tokenized_text = tokenizer.encode(src, return_tensors=\"pt\").to(device)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["src = [\"Indonesia phỏng đoán nguyên nhân tàu ngầm chở 53 người mất tích bí ẩn\"\n","tokenized_text = tokenizer.encode(src, return_tensors=\"pt\").to(device)\n","model.eval()\n","summary_ids = model.generate(\n","                    tokenized_text,\n","                    max_length=512, \n","                    num_beams=5,\n","                    repetition_penalty=2.5, \n","                    length_penalty=1.0, \n","                    early_stopping=True\n","                )\n","output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","print(output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22gqyAO_gg5x"},"outputs":[],"source":["import math\n","batch_size = 16\n","batches = []\n","for i in range(math.ceil(len(sentences_vi) / batch_size)):\n","    if i == math.ceil(len(sentences_vi) / batch_size):\n","        batches.append(sentences_vi[i * batch_size:])\n","    else:\n","        batches.append(sentences_vi[i * batch_size: (i+1) * batch_size])\n","\n","batches = [[sent.strip() for sent in batch] for batch in batches]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1655326081792,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"xgzUWPppgrhJ","outputId":"5e815163-15a0-40f4-b91e-37219bcfdf81"},"outputs":[{"data":{"text/plain":["['cách để đi',\n"," 'họ đã xét nghiệm máu cho cheng nhưng mọi thứ vẫn hoàn toàn bình thường',\n"," 'anh có thể gọi tôi không',\n"," 'có rất nhiều yếu tố may rủi ở đây',\n"," 'ai là chúa nói dối',\n"," 'có cửa hàng tiện lợi ở sân bay không',\n"," 'anh đổi ngoại tệ được không',\n"," 'cô ấy mua hai mươi trái chôm chôm và ba con cá',\n"," 'đứa bé có ói òng ọc ra sữa hoặc nước và bé chưa đến mười tuần tuổi hoặc có biểu hiện mất nước không',\n"," 'tôi có thể viết tên địa chỉ số điện thoại của công ty bảo hiểm của anh được không',\n"," 'đừng lo tôi vừa mới đến rồi',\n"," 'mười phút nữa vào giờ rưỡi',\n"," 'tôi có thể hẹn cho chiều mai không',\n"," 'anh rửa hình kỹ thuật số ra được không',\n"," 'tôi cố thu xếp thôi',\n"," 'tôi không dùng ma túy']"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["batches[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66},"executionInfo":{"elapsed":1707359,"status":"ok","timestamp":1655327821370,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"SkaeDpohgNlu","outputId":"863dc93c-9225-49f3-f636-230149ff02c9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"485c4e69de624555ba5539c84f936edc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4063 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1706.7108855247498\n"]}],"source":["import time\n","from tqdm import trange\n","\n","start = time.time()\n","with open('back_translation_eng.txt', encoding = 'utf-8', mode = 'a') as f:\n","  pbar = tqdm(total= len(batches))\n","  for i in range(len(batches)):\n","    # if i \u003e= 3112:\n","    sentences = batches[i]\n","    #Tokenize our input sentence using the model's custom tokenizer\n","    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to(device)\n","    translated = model.generate(**inputs.to(device),  max_length=512, \n","                    num_beams=3,\n","                    repetition_penalty=2.5, \n","                    length_penalty=1.0, \n","                    early_stopping=True)\n","    trans = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n","    # print(trans)\n","    for translation in trans:\n","      f.write(translation + '\\n')\n","    pbar.update(1)\n","print(time.time() - start)  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655325975300,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"2wjUsTZxHbqn","outputId":"07e7bccb-56b9-4165-ef51-2f23933d3741"},"outputs":[{"data":{"text/plain":["3112.0"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["49792 / 16"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1655325926641,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"X4ZKb_RAGAvy","outputId":"2bc3e030-b50c-4192-ccc6-d7a0b62e191b"},"outputs":[{"name":"stdout","output_type":"stream","text":["49792\n","China adopted the United Nations Framework Agreement on Tobacco Control on 11 October 2005, and soon issued a series of laws banning smoking in rooms in public places in March 2011.\n","\n"]}],"source":["with open('back_translation_english.txt', encoding = 'utf-8', mode = 'r') as f:\n","  a = f.readlines()[:49792]\n","print(len(a))\n","print(a[-1])\n","\n","with open('back_translation_eng.txt', encoding = 'utf-8', mode = 'w') as f:\n","  for line in a:\n","    f.write(line)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"elapsed":7,"status":"error","timestamp":1655827417054,"user":{"displayName":"Nguyễn Tuấn Dũng","userId":"01958371616315406637"},"user_tz":-420},"id":"1oIWDfaX-16F","outputId":"ef0f2c5d-635f-421f-aee8-a422769ef269"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-10-ab171759f871\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maugmented_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.../.../.../data/processed/augmented/back_translation/back_translation_data.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.../.../.../data/processed/augmented/back_translation/back_translation_data.txt'"]}],"source":["augmented_en = '.../.../.../data/processed/augmented/back_translation/back_translation_data.txt'\n","with open(augmented_en, encoding = 'utf-8', mode = 'r') as f:\n","  print(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"anxLJvA7AZ6v"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"zP-hb6TRuxRK"},"source":["## Since we only perform augmentation on half of the original corpus. We will choose randomly 50% of the sentences and save it to 2 separate files (1 for Vietnamese and 1 for English) to use for augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7JXm5tK5s-Ry"},"outputs":[],"source":["import random\n","random.seed(42)\n","chosen_index = []\n","while chosen_lines \u003c= 65000:\n","  index = random.randint(0,len(sentences_en))\n","  if index not in chosen_index:\n","    chosen_index.append(index)\n","    with open('augment_backtr_vi.txt', mode = 'a', newline = \"\", encoding = 'utf-8') as f:\n","      f.write(sentences_vi[index] )\n","    with open('augment_backtr_en.txt', mode = 'a', newline = \"\", encoding = 'utf-8') as f:\n","      f.write(sentences_en[index] )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRf8IAFN0zqb"},"outputs":[],"source":["augment_backtr = []\n","with open('augment_backtr_en.txt', encoding = 'utf8') as f:\n","    for line in f.readlines():\n","        augment_backtr.append(line)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_jtOoEP0zqc","outputId":"af1470bf-c009-41a9-cd60-a095f4897c57"},"outputs":[{"data":{"text/plain":["65001"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["len(augment_backtr)"]},{"cell_type":"markdown","metadata":{"id":"QzWd3y2cvBhq"},"source":["## EN-FR translation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-x1JoabMTtjb"},"outputs":[],"source":["from transformers import MarianMTModel, MarianTokenizer"]},{"cell_type":"markdown","metadata":{"id":"ylUYJH7fvJV5"},"source":["### Download the EN-FR model and its tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFST0CCATzTR"},"outputs":[],"source":["model_marian = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46xuBURXVw6E","outputId":"0a61e84d-a087-4735-a87a-85c5a5a69d88"},"outputs":[{"data":{"text/plain":["MarianMTModel(\n","  (model): MarianModel(\n","    (shared): Embedding(59514, 512, padding_idx=59513)\n","    (encoder): MarianEncoder(\n","      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (decoder): MarianDecoder(\n","      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model_marian.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ez95SpuOUEyP"},"outputs":[],"source":["tokenizer_marian = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"]},{"cell_type":"markdown","metadata":{"id":"8eqkXmkPvRKF"},"source":["### The MarianMT model requires adding a prefix to the sentence to denote the target language. Here it is French"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zg2tgjSUUOM-"},"outputs":[],"source":["src_en = [ (\"\u003e\u003efra\u003c\u003c \" + sentence) for sentence in augment_backtr ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFQWpFc6V6ky","outputId":"edc4450c-d3f3-43bf-d53a-fddf3a9ad7b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003e\u003efra\u003c\u003c If we can put our anger inside an engine , it can drive us forward , it can get us through the dreadful moments and it can give us real inner power .\n","\n"]}],"source":["print(src_en[11740])"]},{"cell_type":"markdown","metadata":{"id":"oDuysdbL2fuq"},"source":["### Divide our data into batches for inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cch4h2UO0zqd"},"outputs":[],"source":["import math\n","batch_size = 8\n","batches = []\n","for i in range(math.ceil(len(src_en) / batch_size)):\n","    if i == math.ceil(len(src_en) / batch_size):\n","        batches.append(src_en[i * batch_size:])\n","    else:\n","        batches.append(src_en[i * batch_size: (i+1) * batch_size])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOobJSwx0zqe","outputId":"7df878aa-f435-441c-80f3-24ac21284085"},"outputs":[{"data":{"text/plain":["8126"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["len(batches)"]},{"cell_type":"markdown","metadata":{"id":"QxfNUomLwvNb"},"source":["### Translate the sentences and store it in the file france_translation.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"kO86vowwVSer","outputId":"0939cad7-160d-4ca2-fe1f-beae1da36482"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c314f961be604d05b87428c45de949b1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8126 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["11261.03185224533\n"]}],"source":["#4h35p 124, 134, 11668, 48474\n","import time\n","from tqdm import trange\n","\n","start = time.time()\n","with open('france_translation.txt', encoding = 'utf-8', mode = 'a') as f:\n","  pbar = tqdm(total= len(batches))\n","  for i in range(len(batches)):\n","    sentences = batches[i]\n","     #Tokenize our input sentence using the model's custom tokenizer\n","    inputs = tokenizer_marian(sentences, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to(device)\n","    translated = model_marian.generate(**inputs.to(device))\n","    trans = [tokenizer_marian.decode(t, skip_special_tokens=True) for t in translated]\n","    for translation in trans:\n","      f.write(translation + '\\n')\n","    pbar.update(1)\n","print(time.time() - start)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6jM0fc_m7Fp"},"outputs":[],"source":["fr_trans = []\n","with open('france_translation.txt', encoding = 'utf-8') as f:\n","    for line in f.readlines():\n","        fr_trans.append(line)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpZd4g0NnI1v","outputId":"c7649ff6-522d-4d9a-98ea-d314a6de76fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Quelques chercheurs ont demandé à quelques centaines de personnes de lire un article scientifique.\n","\n","\u003e\u003efra\u003c\u003c A couple of researchers asked a few hundred people to read a scientific article .\n","\n"]}],"source":["print(fr_trans[48471])\n","print(src_en[48471])"]},{"cell_type":"markdown","metadata":{"id":"8WvsseBL3IqU"},"source":["## FR - EN translation"]},{"cell_type":"markdown","metadata":{"id":"sC2NiinL0A65"},"source":["### Download the FR-EN model and its tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYA7aSI31Cjp"},"outputs":[],"source":["model_backtr = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IRACnjVAmsz","outputId":"c959f87f-522d-4181-8b14-a6784b0e106d"},"outputs":[{"data":{"text/plain":["MarianMTModel(\n","  (model): MarianModel(\n","    (shared): Embedding(59514, 512, padding_idx=59513)\n","    (encoder): MarianEncoder(\n","      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (decoder): MarianDecoder(\n","      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",")"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["model_backtr.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2kHao4MAIFz"},"outputs":[],"source":["tokenizer_backtr = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")"]},{"cell_type":"markdown","metadata":{"id":"-_sJgRtH3VPY"},"source":["### Add prefix to our sentences like above. This time, our target language is english, so the prefix is \"\u003e\u003eeng\u003c\u003c\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kfFpuOy_lB-"},"outputs":[],"source":["src_fr = [ (\"\u003e\u003eeng\u003c\u003c \" + sentence) for sentence in fr_trans ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blrZh1oa0zqf","outputId":"9d5f0495-5fc1-45be-95de-111c813a2005"},"outputs":[{"data":{"text/plain":["'\u003e\u003eeng\u003c\u003c Trois jambes, trois jambes, trois jambes, mais dans la nature, les insectes ont souvent perdu leurs jambes.\\n'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["src_fr[9627]"]},{"cell_type":"markdown","metadata":{"id":"yycerXZE3h_b"},"source":["### Divide our data into small batches for inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neVAL2qX0zqf"},"outputs":[],"source":["batch_size = 8\n","batches = []\n","for i in range(math.ceil(len(src_fr) / batch_size)):\n","    if i == math.ceil(len(src_fr) / batch_size):\n","        batches.append(src_fr[i * batch_size:])\n","    else:\n","        batches.append(src_fr[i * batch_size: (i+1) * batch_size])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hkv83Vaa0zqg","outputId":"d2f3e7d3-acb8-42fd-ac90-8dd3e58ecdee"},"outputs":[{"data":{"text/plain":["8126"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["len(batches)"]},{"cell_type":"markdown","metadata":{"id":"B68MTicq0Go-"},"source":["### Generate FR-EN translations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"9REf4Dbvm3A_","outputId":"5428ad6c-7873-46f2-c33d-a4199bb03196"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40cb4d9d2ec04234998ff583f8af0d17","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8126 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import time\n","start = time.time()\n","with open('english_backtranslations.txt',encoding = 'utf-8', mode = 'a') as f:\n","  pbar = tqdm(total= len(batches))\n","  for i in range(len(batches)):\n","    sentences = batches[i]\n","\n","    inputs = tokenizer_backtr(sentences, return_tensors=\"pt\", padding=True, max_length = 512, truncation = True).to(device)\n","    translated = model_backtr.generate(**inputs)\n","    trans = [tokenizer_backtr.decode(t, skip_special_tokens=True) for t in translated]\n","    for translation in trans:\n","      f.write(translation + '\\n')\n","    pbar.update(1)\n"]}],"metadata":{"colab":{"collapsed_sections":["zV1oHTJ618sD"],"name":"Back_Translation.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"485c4e69de624555ba5539c84f936edc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c917b71f539493e9aca69c8a9fbd4e6","IPY_MODEL_5f5289d57a7e430ca540ca9b9c327e8d","IPY_MODEL_560914607baf4a95bf1c088c13dc9459"],"layout":"IPY_MODEL_ba90efa48c034de6ad31e8ee40b34c7f"}},"4cc5cb4f8020448d871d0c859c9d62e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"560914607baf4a95bf1c088c13dc9459":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82a3da4bcff84023aa207203ef58b930","placeholder":"​","style":"IPY_MODEL_794abd6e593b404981f5d8acaee8453d","value":" 4063/4063 [28:26\u0026lt;00:00,  1.96s/it]"}},"5f5289d57a7e430ca540ca9b9c327e8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb2e55c897474d39a7be51b525776e5a","max":4063,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bda264ead27c4be6a309ce676d75feb8","value":4063}},"6c917b71f539493e9aca69c8a9fbd4e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_facafdef9fab4f00816785dc5a555869","placeholder":"​","style":"IPY_MODEL_4cc5cb4f8020448d871d0c859c9d62e1","value":"100%"}},"794abd6e593b404981f5d8acaee8453d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82a3da4bcff84023aa207203ef58b930":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba90efa48c034de6ad31e8ee40b34c7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bda264ead27c4be6a309ce676d75feb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"facafdef9fab4f00816785dc5a555869":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb2e55c897474d39a7be51b525776e5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}