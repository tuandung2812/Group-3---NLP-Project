data:
  train:
    src_file: ./data/processed/train/train.en
    tgt_file: ./data/processed/train/train.vi
  dev: 
    src_file: ./data/processed/dev/tst2012.en
    tgt_file: ./data/processed/dev/tst2012.vi

utils:
  vocab:
    en: ./utils/vocab/tokenization/en.json
    vi: ./utils/vocab/tokenization/vi.json
    
  training_runs:
    train_loss: ./utils/runs/tokenization/train_loss
    dev_loss: ./utils/runs/tokenization/dev_loss
    learning_rate: ./utils/runs/tokenization/learning_rate
train:
  num_epochs: 15
  learning_rate: 0.0005
  batch_size: 64
model:
  max_sent_len: 120
  embedding_size: 512
  num_heads: 8
  num_encoder_layers: 3
  num_decoder_layers: 3
  dropout: 0.1
  forward_expansion: 4
  checkpoint_path: ./model/tokenize_model.pth.tar
